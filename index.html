<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"ioa-audio.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Audio &amp; Speech Research Group is a research group under the Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of Sciences. The Principal Investigator of this rese">
<meta property="og:type" content="website">
<meta property="og:title" content="About Audio &amp; Speech Research Group">
<meta property="og:url" content="https://ioa-audio.github.io/index.html">
<meta property="og:site_name" content="About Audio &amp; Speech Research Group">
<meta property="og:description" content="Audio &amp; Speech Research Group is a research group under the Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of Sciences. The Principal Investigator of this rese">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="IOA-Audio">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://ioa-audio.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>About Audio & Speech Research Group</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">About Audio & Speech Research Group</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://ioa-audio.github.io/2022/08/30/about%20us/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="IOA-Audio">
      <meta itemprop="description" content="<br>Audio & Speech Research Group is a research group under the Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of Sciences. <br />The Principal Investigator of this research group is Prof. Feiran Yang (https://people.ucas.ac.cn/~feiran). Our research interests include Blind Source Separation, Array Signal Processing, Voice Anti-Spoofing, Acoustic Echo Cancellation, Speech Enhancement. Contact us: feiran@mail.ioa.ac.cn">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="About Audio & Speech Research Group">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/30/about%20us/" class="post-title-link" itemprop="url">About us</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-30 10:14:35" itemprop="dateCreated datePublished" datetime="2022-08-30T10:14:35+08:00">2022-08-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-09-02 17:01:24" itemprop="dateModified" datetime="2024-09-02T17:01:24+08:00">2024-09-02</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="who-we-are"><a href="#who-we-are" class="headerlink" title="who we are"></a>who we are</h1><p>Audio &amp; Speech Research Group is a research group under the Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of Sciences. The Principal Investigator of this research group is Prof. <a target="_blank" rel="noopener" href="https://people.ucas.ac.cn/~feiran">Feiran Yang</a> (杨飞然). Our research interests include Adaptive Filtering, Blind Source Separation, Array Signal Processing, Voice Anti-Spoofing, Acoustic Echo Cancellation, Speech Enhancement. See <a href="/categories/">HERE</a></p>
<p>The members of group includes Feiran Yang (杨飞然), Yi Wan (万伊), Taihui Wang (王泰辉), Shengdong Liu (刘升东), Zhengqiang Luo (罗正强), Yang Liu (刘杨), Jinfu Wang (王劲夫), Changtao Li (李长涛), Jing Lei (雷菁), Kelan Kuang (匡柯澜), Qing Shi(石擎), Lan Tang (汤澜), Cong Zhang(张聪), Haobo Jia (贾浩博), Dianzhe Ding (丁殿哲).</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://ioa-audio.github.io/2025/12/10/dual-demo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="IOA-Audio">
      <meta itemprop="description" content="<br>Audio & Speech Research Group is a research group under the Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of Sciences. <br />The Principal Investigator of this research group is Prof. Feiran Yang (https://people.ucas.ac.cn/~feiran). Our research interests include Blind Source Separation, Array Signal Processing, Voice Anti-Spoofing, Acoustic Echo Cancellation, Speech Enhancement. Contact us: feiran@mail.ioa.ac.cn">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="About Audio & Speech Research Group">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/12/10/dual-demo/" class="post-title-link" itemprop="url">Dual-Channel Magnitude-Phase Learning with Cross-Attention for Bone-Conducted Speech Restoration</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-12-10 15:14:20 / Modified: 16:10:55" itemprop="dateCreated datePublished" datetime="2025-12-10T15:14:20+08:00">2025-12-10</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Speech-Enhancement/" itemprop="url" rel="index"><span itemprop="name">Speech Enhancement</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Author: Dianzhe Ding<br>Email: <a href="mailto:&#100;&#x69;&#x6e;&#x67;&#100;&#x69;&#97;&#110;&#x7a;&#104;&#x65;&#64;&#x6d;&#x61;&#105;&#x6c;&#x2e;&#105;&#x6f;&#97;&#x2e;&#97;&#x63;&#46;&#99;&#x6e;">&#100;&#x69;&#x6e;&#x67;&#100;&#x69;&#97;&#110;&#x7a;&#104;&#x65;&#64;&#x6d;&#x61;&#105;&#x6c;&#x2e;&#105;&#x6f;&#97;&#x2e;&#97;&#x63;&#46;&#99;&#x6e;</a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>We present some speech samples in the website to show the dual-channel magnitude-phase learning with cross-attention for bone-conducted speech. Objective and subjective evaluations on A4BS dataset show that our system substantially outperforms existing bone-conducted speech enhancement systems.</p>
<h2 id="Samples"><a href="#Samples" class="headerlink" title="Samples"></a>Samples</h2><p>We encourage our readers to listen to the following audio samples in order to experience the audio quality of our enhanced speech. Due to the large number of audio files, the loading time might be prolonged. Kindly wait for a moment or consider closing other web pages and refreshing the website to improve the loading speed.</p>
<table>
  <tbody>
    <tr>
    </tr>
    <tr>
      <td>Sample1</td>
      <td> AC <audio src="https://github.com/dianzheding/samples/raw/refs/heads/main/RAW/speaker_1/ac_1.wav" controls></td>
      <td> BC <audio src="https://github.com/dianzheding/samples/raw/refs/heads/main/RAW/speaker_1/bc_1.wav" controls></td>
      <td> EBEN <audio src="https://github.com/dianzheding/samples/raw/refs/heads/main/RAW/speaker_1/eben_1.wav" controls></td>
      <td> DPT-EGNet <audio src="https://github.com/dianzheding/samples/raw/refs/heads/main/RAW/speaker_1/dpt_1.wav" controls></td>
    </tr>
    <tr>
      <td></td>
      <td> WaveNet <audio src="https://github.com/dianzheding/samples/raw/refs/heads/main/RAW/speaker_1/wavenet_1.wav" controls></td>
      <td> Two Stage Approach <audio src="https://github.com/dianzheding/samples/raw/refs/heads/main/RAW/speaker_1/unet_1.wav" controls></td>
      <td> Proposed Network <audio src="https://github.com/dianzheding/samples/raw/refs/heads/main/RAW/speaker_1/stft_1.wav" controls></td>
      <td></td>
    </tr>
  </tbody>
  <colgroup>
  </colgroup>
</table>

<table>
  <tbody>
    <tr>
    </tr>
    <tr>
      <td>Sample2</td>
      <td> AC <audio src="https://github.com/dianzheding/samples/raw/refs/heads/main/RAW/speaker_2/ac_2.wav" controls></td>
      <td> BC <audio src="https://github.com/dianzheding/samples/raw/refs/heads/main/RAW/speaker_2/bc_2.wav" controls></td>
      <td> EBEN <audio src="https://github.com/dianzheding/samples/raw/refs/heads/main/RAW/speaker_2/eben_2.wav" controls></td>
      <td> DPT-EGNet <audio src="https://github.com/dianzheding/samples/raw/refs/heads/main/RAW/speaker_2/dpt_2.wav" controls></td>
    </tr>
    <tr>
      <td></td>
      <td> WaveNet <audio src="https://github.com/dianzheding/samples/raw/refs/heads/main/RAW/speaker_2/wavenet_2.wav" controls></td>
      <td> Two Stage Approach <audio src="https://github.com/dianzheding/samples/raw/refs/heads/main/RAW/speaker_2/unet_2.wav" controls></td>
      <td> Proposed Network <audio src="https://github.com/dianzheding/samples/raw/refs/heads/main/RAW/speaker_2/stft_2.wav" controls></td>
      <td></td>
    </tr>
  </tbody>
  <colgroup>
  </colgroup>
</table>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/12/10/dual-demo/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://ioa-audio.github.io/2025/06/16/demo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="IOA-Audio">
      <meta itemprop="description" content="<br>Audio & Speech Research Group is a research group under the Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of Sciences. <br />The Principal Investigator of this research group is Prof. Feiran Yang (https://people.ucas.ac.cn/~feiran). Our research interests include Blind Source Separation, Array Signal Processing, Voice Anti-Spoofing, Acoustic Echo Cancellation, Speech Enhancement. Contact us: feiran@mail.ioa.ac.cn">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="About Audio & Speech Research Group">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/06/16/demo/" class="post-title-link" itemprop="url">Multi-Modal Speech Enhancement with BiNet and Contrastive Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-06-16 10:49:19" itemprop="dateCreated datePublished" datetime="2025-06-16T10:49:19+08:00">2025-06-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-03-13 10:18:18" itemprop="dateModified" datetime="2024-03-13T10:18:18+08:00">2024-03-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Speech-Enhancement/" itemprop="url" rel="index"><span itemprop="name">Speech Enhancement</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Author: Changtao Li<br>Email: <a href="mailto:&#108;&#x69;&#x63;&#x68;&#97;&#x6e;&#x67;&#116;&#97;&#111;&#x40;&#109;&#x61;&#x69;&#108;&#x2e;&#x69;&#111;&#x61;&#x2e;&#x61;&#x63;&#x2e;&#x63;&#x6e;">&#108;&#x69;&#x63;&#x68;&#97;&#x6e;&#x67;&#116;&#97;&#111;&#x40;&#109;&#x61;&#x69;&#108;&#x2e;&#x69;&#111;&#x61;&#x2e;&#x61;&#x63;&#x2e;&#x63;&#x6e;</a></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>This paper investigates the joint use of bone-conducted and air-conducted speech in a multimodal speech enhancement framework. Starting from the backbone network architecture, we design a temporal two-tower network named after BiNet, capable of directly processing both bone-conducted and noisy air-conducted speech inputs. BiNet employs two independent encoders to map bone-conducted and noisy air-conducted speech into a shared embedding space. A decoder is then utilized to reconstruct the target clean speech from the embedding features of both modalities. Additionally, skip connections are incorporated in BiNet to better capture the long-term and short-term temporal correlations in speech. Considering the significance of spectral components in speech perception, we adopt a multi-scale mel-spectrogram loss function as the training objective, which encourages the network to generate more plausible spectral details of the desired speech. The aforementioned backbone network design allows us to apply regularization constraints based on contrastive learning. By controlling the similarity between the embedding features of bone-conducted and noisy air-conducted speech, we impose two regularization constraints on BiNet. When the embedding features of these two modalities exhibit higher similarity, the proposed BiNet achieves superior speech enhancement performance. Extensive experiments conducted on a recorded dataset of bone-conducted/air-conducted speech validate our approach. By combining the proposed model with contrastive learning regularization constraints, our method outperforms baseline models and several recent multimodal speech enhancement systems in terms of PESQ and STOI metrics. </p>
<h2 id="Samples"><a href="#Samples" class="headerlink" title="Samples"></a>Samples</h2><p>We encourage our readers to listen to the following audio samples in order to experience the audio quality of our enhanced speech.</p>
<table>
  <tbody>
    <tr>
    </tr>
    <tr>
      <td>-10 dB</td>
      <td> air-conducted speech <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/min_10/clean.wav" controls></td>
      <td> bone-conducted speech <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/min_10/bone.wav" controls></td>
      <td> noisy speech <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/min_10/noisy.wav" controls></td>
      <td> UNet restoration <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/min_10/res.wav" controls></td>
      <td> UNet denoising<audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/min_10/dn.wav" controls></td>
    </tr>
    <tr>
      <td></td>
      <td> DCCRN <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/min_10/dccrn.wav" controls></td>   
      <td> UNet early fusion <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/min_10/simple_fuse.wav" controls></td>
      <td> involution <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/min_10/inv.wav" controls></td>
      <td> BiNet w/o regularization <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/min_10/tt.wav" controls></td>
      <td> BiNet w/ regularization <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/min_10/tt_clip.wav" controls></td>     
      <td></td>
    </tr>
    </tbody>
  <colgroup>
  </colgroup>
</table>

<table>
  <tbody>
    <tr>
    </tr>
    <tr>
      <td>-5 dB</td>
      <td> air-conducted speech <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/min_5/clean.wav" controls></td>
      <td> bone-conducted speech <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/min_5/bone.wav" controls></td>
      <td> noisy speech <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/min_5/noisy.wav" controls></td>
      <td> UNet restoration <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/min_5/res.wav" controls></td>
      <td> UNet denoising<audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/min_5/dn.wav" controls></td>
    </tr>
    <tr>
      <td></td>
      <td> DCCRN <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/min_5/dccrn.wav" controls></td>   
      <td> UNet early fusion <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/min_5/simple_fuse.wav" controls></td>
      <td> involution <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/min_5/inv.wav" controls></td>
      <td> BiNet w/o regularization <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/min_5/tt.wav" controls></td>
      <td> BiNet w/ regularization <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/min_5/tt_clip.wav" controls></td>     
      <td></td>
    </tr>
    </tr>
  </tbody>
  <colgroup>
  </colgroup>
</table>

<table>
  <tbody>
    <tr>
    </tr>
    <tr>
      <td>0 dB</td>
      <td> air-conducted speech <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/zero/clean.wav" controls></td>
      <td> bone-conducted speech <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/zero/bone.wav" controls></td>
      <td> noisy speech <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/zero/noisy.wav" controls></td>
      <td> UNet restoration <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/zero/res.wav" controls></td>
      <td> UNet denoising<audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/zero/dn.wav" controls></td>
    </tr>
    <tr>
      <td></td>
      <td> DCCRN <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/zero/dccrn.wav" controls></td>   
      <td> UNet early fusion <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/zero/simple_fuse.wav" controls></td>
      <td> involution <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/zero/inv.wav" controls></td>
      <td> BiNet w/o regularization <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/zero/tt.wav" controls></td>
      <td> BiNet w/ regularization <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/zero/tt_clip.wav" controls></td>     
      <td></td>
    </tr>
    </tbody>
  <colgroup>
  </colgroup>
</table>

<table>
  <tbody>
    <tr>
    </tr>
    <tr>
      <td>5 dB</td>
      <td> air-conducted speech <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/pos_5/clean.wav" controls></td>
      <td> bone-conducted speech <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/pos_5/bone.wav" controls></td>
      <td> noisy speech <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/pos_5/noisy.wav" controls></td>
      <td> UNet restoration <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/pos_5/res.wav" controls></td>
      <td> UNet denoising<audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/pos_5/dn.wav" controls></td>
    </tr>
    <tr>
      <td></td>
      <td> DCCRN <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/pos_5/dccrn.wav" controls></td>   
      <td> UNet early fusion <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/pos_5/simple_fuse.wav" controls></td>
      <td> involution <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/pos_5/inv.wav" controls></td>
      <td> BiNet w/o regularization <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/pos_5/tt.wav" controls></td>
      <td> BiNet w/ regularization <audio src="https://github.com/changtaoli/mulit-modal-clip/raw/main/pos_5/tt_clip.wav" controls></td>     
      <td></td>
    </tr>
    </tbody>
  <colgroup>
  </colgroup>
</table>


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://ioa-audio.github.io/2025/06/16/%E4%B8%BB%E8%A7%82%E6%B5%8B%E8%AF%95-online/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="IOA-Audio">
      <meta itemprop="description" content="<br>Audio & Speech Research Group is a research group under the Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of Sciences. <br />The Principal Investigator of this research group is Prof. Feiran Yang (https://people.ucas.ac.cn/~feiran). Our research interests include Blind Source Separation, Array Signal Processing, Voice Anti-Spoofing, Acoustic Echo Cancellation, Speech Enhancement. Contact us: feiran@mail.ioa.ac.cn">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="About Audio & Speech Research Group">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/06/16/%E4%B8%BB%E8%A7%82%E6%B5%8B%E8%AF%95-online/" class="post-title-link" itemprop="url">subjective test of 4 enhanced speech</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-06-16 10:49:19" itemprop="dateCreated datePublished" datetime="2025-06-16T10:49:19+08:00">2025-06-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-05-24 10:05:00" itemprop="dateModified" datetime="2024-05-24T10:05:00+08:00">2024-05-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Speech-Enhancement/" itemprop="url" rel="index"><span itemprop="name">Speech Enhancement</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Author: Kelan Kuang<br>Email: <a href="mailto:&#107;&#117;&#x61;&#110;&#103;&#x6b;&#101;&#108;&#97;&#110;&#64;&#109;&#97;&#105;&#108;&#x2e;&#x69;&#x6f;&#97;&#46;&#97;&#99;&#x2e;&#x63;&#110;">&#107;&#117;&#x61;&#110;&#103;&#x6b;&#101;&#108;&#97;&#110;&#64;&#109;&#97;&#105;&#108;&#x2e;&#x69;&#x6f;&#97;&#46;&#97;&#99;&#x2e;&#x63;&#110;</a></p>
<h1 id="音频文件质量主观评价测试"><a href="#音频文件质量主观评价测试" class="headerlink" title="音频文件质量主观评价测试"></a>音频文件质量主观评价测试</h1><p>感谢参加本次音频文件主观测试。本次测试分为两个部分，首先是对来自不同系统的同一条语音进行打分（mean opinion score；MOS）；接着受试者需要从两条对比的语音中选择质量更好的一条（paired comparison test）。这两部分测试的具体内容会在每一部分测试的开始进行具体介绍。</p>
<h2 id="MOS"><a href="#MOS" class="headerlink" title="MOS"></a>MOS</h2><table>
  <tbody>
    <tr>
    </tr>
    <tr>
      <td>基准语音</td>
      <td> 1分 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/MOS1__33_snr-13.21dB_noisyR0.95_ch1.wav" controls></td>
      <td> 2分 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/MOS2__3_snr-14.68dB_noisyR0.81_out.wav" controls></td>
      <td> 3-4分 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/MOS34__29_snr5.57dB_noisyR0.82_out.wav" controls></td>
      <td> 5分 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/MOS5__3_snr-14.68dB_noisyR0.81_tgt.wav" controls></td>
  </tbody>
  <colgroup>
  </colgroup>
</table>



<p>请你对以下语音的可懂度进行打分，打分范围是1-5。其中5分表示语音可懂度很高，主观上完全可以接受，语音完全不刺耳。4分表示对语音的接受程度降低，但不反感该语音。3分表示可以感知到此语音的瑕疵，如电音等刺耳瑕疵。2分表示该语音的刺耳瑕疵非常明显。1分表示完全无法接受该语音且非常反感。我们给出了1分与5分的语音作为参考，你只需要聆听这些语音以获得语音质量的直观感受。请您以这两条语音作为基准，对以下语音样本进行打分。需要说明的是本次测试是为了对语音质量以及可懂度而非语音的内容进行评判，某些语音的口音可能比较重，请您忽略口音相关的问题。下面的每一个Group，例如Group1，包含来自4个不同的语音增强系统（s1-s4）的输出语音，请在评分后将分数记录在对应的Excel表格中。</p>
<p>极低信噪比: </p>
<table>
  <tbody>
    <tr>
    </tr>
    <tr>
      <td>Group1</td>
      <td> s1 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\1093\FCN_nonCausal_(01)__1093_snr-13.61dB_noisyR0.85_out.wav" controls></td>
      <td> s2 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\1093\ConvTasNet_(01)__1093_snr-13.61dB_noisyR0.85_out.wav" controls></td>
      <td> s3 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\1093\DC_CRN_iAFF_(01)__1093_snr-13.61dB_noisyR0.85_out.wav" controls></td>
      <td> s4 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\1093\DenGCAN_iag2_0_(01)__1093_snr-13.61dB_noisyR0.85_out.wav" controls></td>
    </tr>
  </tbody>
  <colgroup>
  </colgroup>
</table>

<table>
  <tbody>
    <tr>
    </tr>
    <tr>
      <td>Group2</td>
      <td> s1 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\5221\FCN_nonCausal_(01)__5221_snr-14.96dB_noisyR0.82_out.wav" controls></td>
      <td> s2 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\5221\ConvTasNet_(01)__5221_snr-14.96dB_noisyR0.82_out.wav" controls></td>
      <td> s3 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\5221\DC_CRN_iAFF_(01)__5221_snr-14.96dB_noisyR0.82_out.wav" controls></td>
      <td> s4 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\5221\DenGCAN_iag2_0_(01)__5221_snr-14.96dB_noisyR0.82_out.wav" controls></td>
    </tr>
  </tbody>
  <colgroup>
  </colgroup>
</table>

<table>
  <tbody>
    <tr>
    </tr>
    <tr>
      <td>Group3</td>
      <td> s1 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\7092\FCN_nonCausal_(01)__7092_snr-12.86dB_noisyR0.81_out.wav" controls></td>
      <td> s2 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\7092\ConvTasNet_(01)__7092_snr-12.86dB_noisyR0.81_out.wav" controls></td>
      <td> s3 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\7092\DC_CRN_iAFF_(01)__7092_snr-12.86dB_noisyR0.81_out.wav" controls></td>
      <td> s4 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\7092\DenGCAN_iag2_0_(01)__7092_snr-12.86dB_noisyR0.81_out.wav" controls></td>
    </tr>
  </tbody>
  <colgroup>
  </colgroup>
</table>

<table>
  <tbody>
    <tr>
    </tr>
    <tr>
      <td>Group4</td>
      <td> s1 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\8214\FCN_nonCausal_(01)__8214_snr-13.32dB_noisyR0.87_out.wav" controls></td>
      <td> s2 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\8214\ConvTasNet_(01)__8214_snr-13.32dB_noisyR0.87_out.wav" controls></td>
      <td> s3 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\8214\DC_CRN_iAFF_(01)__8214_snr-13.32dB_noisyR0.87_out.wav" controls></td>
      <td> s4 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\8214\DenGCAN_iag2_0_(01)__8214_snr-13.32dB_noisyR0.87_out.wav" controls></td>
    </tr>
  </tbody>
  <colgroup>
  </colgroup>
</table>

<table>
  <tbody>
    <tr>
    </tr>
    <tr>
      <td>Group5</td>
      <td> s1 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\13082\FCN_nonCausal_(01)__13082_snr-9.38dB_noisyR0.81_out.wav" controls></td>
      <td> s2 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\13082\ConvTasNet_(01)__13082_snr-9.38dB_noisyR0.81_out.wav" controls></td>
      <td> s3 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\13082\DC_CRN_iAFF_(01)__13082_snr-9.38dB_noisyR0.81_out.wav" controls></td>
      <td> s4 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\13082\DenGCAN_iag2_0_(01)__13082_snr-9.38dB_noisyR0.81_out.wav" controls></td>
    </tr>
  </tbody>
  <colgroup>
  </colgroup>
</table>

<table>
  <tbody>
    <tr>
    </tr>
    <tr>
      <td>Group6</td>
      <td> s1 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\13759\FCN_nonCausal_(01)__13759_snr-10.92dB_noisyR1.00_out.wav" controls></td>
      <td> s2 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\13759\ConvTasNet_(01)__13759_snr-10.92dB_noisyR1.00_out.wav" controls></td>
      <td> s3 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\13759\DC_CRN_iAFF_(01)__13759_snr-10.92dB_noisyR1.00_out.wav" controls></td>
      <td> s4 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\13759\DenGCAN_iag2_0_(01)__13759_snr-10.92dB_noisyR1.00_out.wav" controls></td>
    </tr>
  </tbody>
  <colgroup>
  </colgroup>
</table>

<table>
  <tbody>
    <tr>
    </tr>
    <tr>
      <td>Group7</td>
      <td> s1 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\20754\FCN_nonCausal_(01)__20754_snr-13.13dB_noisyR0.91_out.wav" controls></td>
      <td> s2 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\20754\ConvTasNet_(01)__20754_snr-13.13dB_noisyR0.91_out.wav" controls></td>
      <td> s3 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\20754\DC_CRN_iAFF_(01)__20754_snr-13.13dB_noisyR0.91_out.wav" controls></td>
      <td> s4 <audio src="https://github.com/CaA23187/DenGCAN_demo/raw/main/snr_-15_-9\20754\DenGCAN_iag2_0_(01)__20754_snr-13.13dB_noisyR0.91_out.wav" controls></td>
    </tr>
  </tbody>
  <colgroup>
  </colgroup>
</table>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/06/16/%E4%B8%BB%E8%A7%82%E6%B5%8B%E8%AF%95-online/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://ioa-audio.github.io/2023/09/15/two-stage-demo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="IOA-Audio">
      <meta itemprop="description" content="<br>Audio & Speech Research Group is a research group under the Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of Sciences. <br />The Principal Investigator of this research group is Prof. Feiran Yang (https://people.ucas.ac.cn/~feiran). Our research interests include Blind Source Separation, Array Signal Processing, Voice Anti-Spoofing, Acoustic Echo Cancellation, Speech Enhancement. Contact us: feiran@mail.ioa.ac.cn">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="About Audio & Speech Research Group">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/09/15/two-stage-demo/" class="post-title-link" itemprop="url">A Two-Stage Approach to Quality Restoration of Bone-Conducted Speech</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-09-15 09:12:21" itemprop="dateCreated datePublished" datetime="2023-09-15T09:12:21+08:00">2023-09-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-09-19 10:06:50" itemprop="dateModified" datetime="2023-09-19T10:06:50+08:00">2023-09-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Speech-Enhancement/" itemprop="url" rel="index"><span itemprop="name">Speech Enhancement</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Author: Changtao Li<br>Email: <a href="mailto:&#108;&#105;&#x63;&#x68;&#97;&#x6e;&#103;&#x74;&#97;&#x6f;&#64;&#x6d;&#97;&#x69;&#x6c;&#x2e;&#105;&#111;&#97;&#x2e;&#x61;&#99;&#x2e;&#99;&#x6e;">&#108;&#105;&#x63;&#x68;&#97;&#x6e;&#103;&#x74;&#97;&#x6f;&#64;&#x6d;&#97;&#x69;&#x6c;&#x2e;&#105;&#111;&#97;&#x2e;&#x61;&#99;&#x2e;&#99;&#x6e;</a></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Bone-conducted speech is not susceptible to background noise but suffers from poor speech quality and intelligibility due to the limited bandwidth. This paper proposes a two-stage approach to restore the quality of bone-conducted speech, namely, bandwidth extension and speech vocoder. In the first stage, a deep neural network is trained to learn mappings from a low-resolution representation of the bone-conducted speech, i.e., log Mel-scale spectrogram, to that of the air-conducted speech, which extends the bandwidth of the bone-conducted speech. In the second stage, a speech vocoder is employed to transform the extended log Mel-scale spectrogram of the bone-conducted speech back to time-domain waveforms. Due to the many-to-many correspondence between the air-conducted and bone-conducted speech, supervised learning may not be the best training protocol for the bone-conducted/air-conducted feature mapping. We thus propose to leverage adversarial training to further improve the bandwidth extension performance in the first stage. The two stages are decoupled and can be trained independently. The vocoder is trained on a large multi-speaker dataset and can generalize well to unknown speakers. Also, the vocoder can help to remedy the spectral artifacts introduced in the bandwidth extension stage. Objective and subjective evaluations on ESMB dataset show that the proposed two-stage system substantially outperforms existing bone-conducted speech enhancement systems. </p>
<h2 id="Samples"><a href="#Samples" class="headerlink" title="Samples"></a>Samples</h2><p>We encourage our readers to listen to the following audio samples in order to experience the audio quality of our enhanced speech. Due to the large number of audio files, the loading time might be prolonged. Kindly wait for a moment or consider closing other web pages and refreshing the website to improve the loading speed.</p>
<table>
  <tbody>
    <tr>
    </tr>
    <tr>
      <td>Sample1</td>
      <td> DCCRN-AF <audio src="https://github.com/changtaoli/samples/raw/main/dccrn_-10/speaker_1/sample_0.wav" controls></td>
      <td> DCCRN-BC <audio src="https://github.com/changtaoli/samples/raw/main/dccrn_stft/speaker_1/sample_0.wav" controls></td>
      <td> S2 <audio src="https://github.com/changtaoli/samples/raw/main/direct/speaker_1/sample_0.wav" controls></td>
      <td> DPT-EGNet <audio src="https://github.com/changtaoli/samples/raw/main/dpt/speaker_1/sample_0.wav" controls></td>
    </tr>
    <tr>
      <td></td>
      <td> Proposed <audio src="https://github.com/changtaoli/samples/raw/main/ours/speaker_1/sample_0.wav" controls></td>
      <td> S1 <audio src="https://github.com/changtaoli/samples/raw/main/unet_stft/speaker_1/sample_0.wav" controls></td>
      <td> WaveNet <audio src="https://github.com/changtaoli/samples/raw/main/wavenet_-10/speaker_1/sample_0.wav" controls></td>   
      <td> BC <audio src="https://github.com/changtaoli/samples/raw/main/bc/speaker_1/bc_0.wav" controls></td>    
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td> AC <audio src="https://github.com/changtaoli/samples/raw/main/ac/speaker_1/ac_0.wav" controls></td>
      <td></td>
    </tr>
  </tbody>
  <colgroup>
  </colgroup>
</table>

<table>
  <tbody>
    <tr>
    </tr>
    <tr>
      <td>Sample2</td>
      <td> DCCRN-AF <audio src="https://github.com/changtaoli/samples/raw/main/dccrn_-10/speaker_13/sample_22.wav" controls></td>
      <td> DCCRN-BC <audio src="https://github.com/changtaoli/samples/raw/main/dccrn_stft/speaker_13/sample_22.wav" controls></td>
      <td> S2 <audio src="https://github.com/changtaoli/samples/raw/main/direct/speaker_13/sample_22.wav" controls></td>
      <td> DPT-EGNet <audio src="https://github.com/changtaoli/samples/raw/main/dpt/speaker_13/sample_22.wav" controls></td>
    </tr>
    <tr>
      <td></td>
      <td> Proposed <audio src="https://github.com/changtaoli/samples/raw/main/ours/speaker_13/sample_22.wav" controls></td>
      <td> S1 <audio src="https://github.com/changtaoli/samples/raw/main/unet_stft/speaker_13/sample_22.wav" controls></td>
      <td> WaveNet <audio src="https://github.com/changtaoli/samples/raw/main/wavenet_-10/speaker_13/sample_22.wav" controls></td>      
      <td> BC <audio src="https://github.com/changtaoli/samples/raw/main/bc/speaker_13/bc_22.wav" controls></td>    
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td> AC <audio src="https://github.com/changtaoli/samples/raw/main/ac/speaker_13/ac_22.wav" controls></td>
      <td></td>
    </tr>
  </tbody>
  <colgroup>
  </colgroup>
</table>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2023/09/15/two-stage-demo/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://ioa-audio.github.io/2023/06/14/best_sup_mdoel/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="IOA-Audio">
      <meta itemprop="description" content="<br>Audio & Speech Research Group is a research group under the Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of Sciences. <br />The Principal Investigator of this research group is Prof. Feiran Yang (https://people.ucas.ac.cn/~feiran). Our research interests include Blind Source Separation, Array Signal Processing, Voice Anti-Spoofing, Acoustic Echo Cancellation, Speech Enhancement. Contact us: feiran@mail.ioa.ac.cn">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="About Audio & Speech Research Group">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/14/best_sup_mdoel/" class="post-title-link" itemprop="url">Restoration of Bone-Conducted Speech with U-Net-like Model and Energy Distance Loss</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-06-14 19:12:21" itemprop="dateCreated datePublished" datetime="2023-06-14T19:12:21+08:00">2023-06-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-07-04 14:16:42" itemprop="dateModified" datetime="2023-07-04T14:16:42+08:00">2023-07-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Speech-Enhancement/" itemprop="url" rel="index"><span itemprop="name">Speech Enhancement</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Author: Changtao Li<br>Email: <a href="mailto:&#x6c;&#x69;&#99;&#104;&#97;&#x6e;&#x67;&#x74;&#97;&#111;&#64;&#x6d;&#x61;&#105;&#x6c;&#x2e;&#105;&#x6f;&#x61;&#x2e;&#97;&#x63;&#x2e;&#99;&#x6e;">&#x6c;&#x69;&#99;&#104;&#97;&#x6e;&#x67;&#x74;&#97;&#111;&#64;&#x6d;&#x61;&#105;&#x6c;&#x2e;&#105;&#x6f;&#x61;&#x2e;&#97;&#x63;&#x2e;&#99;&#x6e;</a></p>
<h1 id="Restoration-of-Bone-Conducted-Speech-with-U-Net-like-Model-and-Energy-Distance-Loss"><a href="#Restoration-of-Bone-Conducted-Speech-with-U-Net-like-Model-and-Energy-Distance-Loss" class="headerlink" title="Restoration of Bone-Conducted Speech with U-Net-like Model and Energy Distance Loss"></a>Restoration of Bone-Conducted Speech with U-Net-like Model and Energy Distance Loss</h1><h2 id="Audio-samples"><a href="#Audio-samples" class="headerlink" title="Audio samples"></a>Audio samples</h2><p>We encourage our readers to listen to the following audio samples in order to experience the audio quality of our enhanced speech.</p>
<table>
  <tbody>
    <tr>
        <th>Signals </th>
        <th>sample1</th>
        <th>sample2</th>
        <th>sample3</th>
        <th>sample4</th>
    </tr>
    <tr>
      <td>Bone-conducted Speech</td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/bc/bc_11.wav" controls></td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/bc/bc_26.wav" controls></td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/bc/bc_41.wav" controls></td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/bc/bc_47.wav" controls></td>
    </tr>
    <tr>
      <td>Air-conducted Speech</td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/air/ac_11.wav" controls></td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/air/ac_26.wav" controls></td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/air/ac_41.wav" controls></td>      
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/air/ac_47.wav" controls></td>
    </tr>
    <tr>
      <td>DPT-EGNet</td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/dpt-egnet/sample11.wav" controls></td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/dpt-egnet/sample26.wav" controls></td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/dpt-egnet/sample41.wav" controls></td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/dpt-egnet/sample47.wav" controls></td>
    </tr>
    <tr>
      <td>EBEN</td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/eben/sample11.wav" controls></td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/eben/sample26.wav" controls></td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/eben/sample41.wav" controls></td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/eben/sample47.wav" controls></td>
    </tr>
    <tr>
      <td>Ours (l1 loss)</td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/new_l1/sample11.wav" controls></td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/new_l1/sample26.wav" controls></td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/new_l1/sample41.wav" controls></td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/new_l1/sample47.wav" controls></td>
    </tr>
    <tr>
      <td>Ours (energy loss)</td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/enenrgy/sample_11.wav" controls></td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/enenrgy/sample_26.wav" controls></td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/enenrgy/sample_41.wav" controls></td>
      <td><audio src="https://github.com/changtaoli/model-energy/raw/main/enenrgy/sample_47.wav" controls></td>
    </tr>
  </tbody>
  <colgroup>
    <col style="width: 5%;">
    <col style="width: 5%;">
    <col style="width: 5%;">
    <col style="width: 5%;">
  </colgroup>
</table>


<h2 id="More-samples"><a href="#More-samples" class="headerlink" title="More samples"></a>More samples</h2><p>For more samples,  See <a target="_blank" rel="noopener" href="https://github.com/changtaoli/model-energy">https://github.com/changtaoli/model-energy</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://ioa-audio.github.io/2023/03/27/CTFMNMF_demo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="IOA-Audio">
      <meta itemprop="description" content="<br>Audio & Speech Research Group is a research group under the Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of Sciences. <br />The Principal Investigator of this research group is Prof. Feiran Yang (https://people.ucas.ac.cn/~feiran). Our research interests include Blind Source Separation, Array Signal Processing, Voice Anti-Spoofing, Acoustic Echo Cancellation, Speech Enhancement. Contact us: feiran@mail.ioa.ac.cn">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="About Audio & Speech Research Group">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/03/27/CTFMNMF_demo/" class="post-title-link" itemprop="url">CTF-MNMF demo</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-03-27 14:25:21" itemprop="dateCreated datePublished" datetime="2023-03-27T14:25:21+08:00">2023-03-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-04-04 10:09:12" itemprop="dateModified" datetime="2023-04-04T10:09:12+08:00">2023-04-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Blind-Source-Separation/" itemprop="url" rel="index"><span itemprop="name">Blind Source Separation</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>作者: Taihui Wang<br>Email: <a href="mailto:&#119;&#97;&#x6e;&#x67;&#x74;&#x61;&#x69;&#x68;&#x75;&#105;&#64;&#x6d;&#x61;&#105;&#108;&#x2e;&#105;&#111;&#x61;&#x2e;&#x61;&#99;&#x2e;&#99;&#110;">&#119;&#97;&#x6e;&#x67;&#x74;&#x61;&#x69;&#x68;&#x75;&#105;&#64;&#x6d;&#x61;&#105;&#108;&#x2e;&#105;&#111;&#x61;&#x2e;&#x61;&#99;&#x2e;&#99;&#110;</a></p>
<h1 id="Convolutive-Transfer-Function-Based-Multichannel-Nonnegative-Matrix-Factorization-for-Overdetermined-Blind-Source-Separation"><a href="#Convolutive-Transfer-Function-Based-Multichannel-Nonnegative-Matrix-Factorization-for-Overdetermined-Blind-Source-Separation" class="headerlink" title="Convolutive Transfer Function-Based Multichannel Nonnegative Matrix Factorization for Overdetermined Blind Source Separation"></a>Convolutive Transfer Function-Based Multichannel Nonnegative Matrix Factorization for Overdetermined Blind Source Separation</h1><p>Taihui Wang <sup>1</sup><sup>, 2</sup>  , Feiran Yang <sup>3</sup>  , Member, IEEE, and Jun Yang <sup>1</sup><sup>, 2</sup>  , Senior Member, IEEE</p>
<p><sup>1</sup>  Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of Sciences, Beijing,<br>China</p>
<p> <sup>2</sup> University of Chinese Academy of Sciences, Beijing, China</p>
<p> <sup>3</sup> State Key Laboratory of Acoustics, Institute of Acoustics, Chinese Academy of Sciences, Beijing, China</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Most multichannel blind source separation (BSS) approaches rely on a spatial model to encode the transfer functions from sources to microphones and a source model to encode the source power spectral density. The rank-1 spatial model has been widely exploited in independent component analysis (ICA), independent vector analysis (IVA), and independent low-rank matrix analysis (ILRMA). The full-rank spatial model is also considered in many BSS approaches, such as full-rank spatial covariance matrix analysis (FCA), multichannel nonnegative matrix factorization (MNMF), and FastMNMF, which can improve the separation performance in the case of long reverberation times. This paper proposes a new MNMF framework based on the convolutive transfer function (CTF) for overdetermined BSS. The time-domain convolutive mixture model is approximated by a frequency-wise convolutive mixture model instead of the widely adopted frequency-wise instantaneous mixture model. The iterative projection algorithm is adopted to estimate the demixing matrix, and the multiplicative update rule is employed to estimate nonnegative matrix factorization (NMF) parameters. Finally, the source image is reconstructed using a multichannel Wiener filter. The advantages of the proposed method are twofold. First, the CTF approximation enables us to use a short window to represent long impulse responses. Second, the full-rank spatial model can be derived based on the CTF approximation and slowly time-variant source variances, and close relationships between the proposed method and ILRMA, FCA, MNMF and FastMNMF are revealed. Extensive experiments show that the proposed algorithm achieves a higher separation performance than ILRMA and FastMNMF in reverberant environments.</p>
<h2 id="Separated-audio-samples"><a href="#Separated-audio-samples" class="headerlink" title="Separated audio samples"></a>Separated audio samples</h2><p>The following table shows an example of the 2-music source separation task, where the public NMF model is used and the number of bases is set to 32.  The reverberation time is 470 ms, and the channel number is 6. </p>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-baqh{text-align:center;vertical-align:top}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-0pky"></th>
    <th class="tg-c3ow">Source 1</th>
    <th class="tg-c3ow">Source 2</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-c3ow">Clean reverberant sources</td>
    <td class="tg-c3ow"><audio src="https://github.com/TaihuiWang/SeparatedAudioSamplesUsingCTF-MNMF/blob/main/musicSignals/2-musicSourceUsing6micsWithReverberationTime470ms/image1.wav?raw=true" controls></td>
    <td class="tg-c3ow"><audio src="https://github.com/TaihuiWang/SeparatedAudioSamplesUsingCTF-MNMF/blob/main/musicSignals/2-musicSourceUsing6micsWithReverberationTime470ms/image2.wav?raw=true" controls></td>
  </tr>
  <tr>
    <td class="tg-c3ow">Mixture at the first microphone</td>
    <td class="tg-c3ow" colspan="2"><audio src="https://github.com/TaihuiWang/SeparatedAudioSamplesUsingCTF-MNMF/blob/main/musicSignals/2-musicSourceUsing6micsWithReverberationTime470ms/mixture.wav?raw=true" controls></td>
  </tr>
  <tr>
    <td class="tg-baqh">Separated sources uing CTF-MNMF</td>
    <td class="tg-baqh"><audio src="https://github.com/TaihuiWang/SeparatedAudioSamplesUsingCTF-MNMF/blob/main/musicSignals/2-musicSourceUsing6micsWithReverberationTime470ms/sep1UsingCTFMNMF-SDR.wav?raw=true" controls></td>
    <td class="tg-baqh"><audio src="https://github.com/TaihuiWang/SeparatedAudioSamplesUsingCTF-MNMF/blob/main/musicSignals/2-musicSourceUsing6micsWithReverberationTime470ms/sep2UsingCTFMNMF-SDR.wav?raw=true" controls></td>
  </tr>
</tbody>
</table>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2023/03/27/CTFMNMF_demo/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://ioa-audio.github.io/2023/03/20/TriU-Net_demo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="IOA-Audio">
      <meta itemprop="description" content="<br>Audio & Speech Research Group is a research group under the Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of Sciences. <br />The Principal Investigator of this research group is Prof. Feiran Yang (https://people.ucas.ac.cn/~feiran). Our research interests include Blind Source Separation, Array Signal Processing, Voice Anti-Spoofing, Acoustic Echo Cancellation, Speech Enhancement. Contact us: feiran@mail.ioa.ac.cn">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="About Audio & Speech Research Group">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/03/20/TriU-Net_demo/" class="post-title-link" itemprop="url">TriU-Net demo</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-03-20 19:12:21" itemprop="dateCreated datePublished" datetime="2023-03-20T19:12:21+08:00">2023-03-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-04-04 10:09:30" itemprop="dateModified" datetime="2023-04-04T10:09:30+08:00">2023-04-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Speech-Enhancement/" itemprop="url" rel="index"><span itemprop="name">Speech Enhancement</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>作者: Kelan Kuang<br>Email: <a href="mailto:&#107;&#117;&#97;&#x6e;&#x67;&#x6b;&#101;&#x6c;&#97;&#110;&#x40;&#x6d;&#x61;&#105;&#108;&#x2e;&#x69;&#x6f;&#x61;&#46;&#97;&#99;&#46;&#x63;&#x6e;">&#107;&#117;&#97;&#x6e;&#x67;&#x6b;&#101;&#x6c;&#97;&#110;&#x40;&#x6d;&#x61;&#105;&#108;&#x2e;&#x69;&#x6f;&#x61;&#46;&#97;&#99;&#46;&#x63;&#x6e;</a></p>
<h1 id="Multi-Channel-Speech-Enhancement-results"><a href="#Multi-Channel-Speech-Enhancement-results" class="headerlink" title="Multi-Channel Speech Enhancement results"></a>Multi-Channel Speech Enhancement results</h1><p>This repo shows enhanced speech of TriU-Net and other models.</p>
<h2 id="Real-recordings"><a href="#Real-recordings" class="headerlink" title="Real recordings"></a>Real recordings</h2><p>SNR: -5 dB</p>
<p>Spk 1 is a 24-year-old male, while spk 2 is a 24-year-old female. Both spk 1 and spk 2 speek in Mandarin. </p>
<table>
  <tbody>
    <tr>
        <th>Signals </th>
        <th>Spk 1 babble</th>
        <th>Spk 2 babble</th>
        <th>Spk 1 f16</th>
        <th>Spk 2 f16</th>
    </tr>
    <tr>
      <td>noisy</td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/9ULA_synth_kkl_babble_snr-5.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk1/babble/9ULA_synth_kkl_babble_snr-5.wav" controls></td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/9ULA_synth_yzh_babble_snr-5.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk2/babble/9ULA_synth_yzh_babble_snr-5.wav" controls></td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/9ULA_synth_kkl_babble_snr-5.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk1/f16/9ULA_synth_kkl_f16_snr-5.wav" controls></td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/9ULA_synth_yzh_f16_snr-5.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk2/f16/9ULA_synth_yzh_f16_snr-5.wav" controls></td>
    </tr>
    <tr>
      <td>clean</td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/9ULA_kkl_clean.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk1/babble/9ULA_kkl_clean.wav" controls></td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/9ULA_yzh_clean.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk2/babble/9ULA_yzh_clean.wav" controls></td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/9ULA_kkl_clean.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk1/f16/9ULA_kkl_clean.wav" controls></td>      
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/9ULA_yzh_clean.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk2/f16/9ULA_yzh_clean.wav" controls></td>
    </tr>
    <tr>
      <td>oracl MB-MVDR</td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/real_enh_oracle_MBMVDR_kkl_babble_snr-5_pesq1.09_1.36.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk1/babble/real_enh_oracle_MBMVDR_kkl_babble_snr-5_pesq1.09_1.36.wav" controls></td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/real_enh_oracle_MBMVDR_yzh_babble_snr-5_pesq1.06_1.27.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk2/babble/real_enh_oracle_MBMVDR_yzh_babble_snr-5_pesq1.06_1.27.wav" controls></td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/real_enh_oracle_MBMVDR_kkl_f16_snr-5_pesq1.06_1.3.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk1/f16/real_enh_oracle_MBMVDR_kkl_f16_snr-5_pesq1.06_1.3.wav" controls></td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/real_enh_oracle_MBMVDR_yzh_f16_snr-5_pesq1.04_1.25.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk2/f16/real_enh_oracle_MBMVDR_yzh_f16_snr-5_pesq1.04_1.25.wav" controls></td>
    </tr>
    <tr>
      <td>EaBNet</td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/real_enh_EaBNet_kkl_babble_snr-5_pesq1.09_1.27.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk1/babble/real_enh_EaBNet_kkl_babble_snr-5_pesq1.09_1.27.wav" controls></td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/real_enh_EaBNet_yzh_babble_snr-5_pesq1.06_1.33.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk2/babble/real_enh_EaBNet_yzh_babble_snr-5_pesq1.06_1.33.wav" controls></td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/real_enh_EaBNet_kkl_f16_snr-5_pesq1.06_1.37.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk1/f16/real_enh_EaBNet_kkl_f16_snr-5_pesq1.06_1.37.wav" controls></td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/real_enh_EaBNet_yzh_f16_snr-5_pesq1.04_1.41.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk2/f16/real_enh_EaBNet_yzh_f16_snr-5_pesq1.04_1.41.wav" controls></td>
    </tr>
    <tr>
      <td>FaSNet</td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/real_enh_FaSNet_kkl_babble_snr-5_pesq1.09_1.24.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk1/babble/real_enh_FaSNet_kkl_babble_snr-5_pesq1.09_1.24.wav" controls></td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/real_enh_FaSNet_yzh_babble_snr-5_pesq1.06_1.23.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk2/babble/real_enh_FaSNet_yzh_babble_snr-5_pesq1.06_1.23.wav" controls></td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/real_enh_FaSNet_kkl_f16_snr-5_pesq1.06_1.35.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk1/f16/real_enh_FaSNet_kkl_f16_snr-5_pesq1.06_1.35.wav" controls></td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/real_enh_FaSNet_yzh_f16_snr-5_pesq1.04_1.47.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk2/f16/real_enh_FaSNet_yzh_f16_snr-5_pesq1.04_1.47.wav" controls></td>
    </tr>
    <tr>
      <td>Model A</td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/real_enh_ModelA_kkl_babble_snr-5_pesq1.09_1.42.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk1/babble/real_enh_ModelA_kkl_babble_snr-5_pesq1.09_1.42.wav" controls></td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/real_enh_ModelA_yzh_babble_snr-5_pesq1.06_1.26.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk2/babble/real_enh_ModelA_yzh_babble_snr-5_pesq1.06_1.26.wav" controls></td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/real_enh_ModelA_kkl_f16_snr-5_pesq1.06_1.6.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk1/f16/real_enh_ModelA_kkl_f16_snr-5_pesq1.06_1.6.wav" controls></td>
      <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/real_enh_ModelA_yzh_f16_snr-5_pesq1.04_1.77.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk2/f16/real_enh_ModelA_yzh_f16_snr-5_pesq1.04_1.77.wav" controls></td>
    </tr>
    <tr>
        <td><b>TriU-Net</b></td>
        <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/real_enh_TriU-Net_kkl_babble_snr-5_pesq1.09_1.7.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk1/babble/real_enh_TriU-Net_kkl_babble_snr-5_pesq1.09_1.7.wav" controls></td>
        <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/real_enh_TriU-Net_yzh_babble_snr-5_pesq1.06_1.65.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk2/babble/real_enh_TriU-Net_yzh_babble_snr-5_pesq1.06_1.65.wav" controls></td>
        <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/real_enh_TriU-Net_kkl_f16_snr-5_pesq1.06_1.84.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk1/f16/real_enh_TriU-Net_kkl_f16_snr-5_pesq1.06_1.84.wav" controls></td>
        <td><img src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/spectrograms/real_enh_TriU-Net_yzh_f16_snr-5_pesq1.04_1.9.jpg"><audio src="https://raw.githubusercontent.com/CaA23187/TriU-Net_demo/main/real%20recording/spk2/f16/real_enh_TriU-Net_yzh_f16_snr-5_pesq1.04_1.9.wav" controls></td>
    </tr>
  </tbody>
  <colgroup>
    <col style="width: 5%;">
    <col style="width: 5%;">
    <col style="width: 5%;">
    <col style="width: 5%;">
  </colgroup>
</table>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2023/03/20/TriU-Net_demo/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">IOA-Audio</p>
  <div class="site-description" itemprop="description"><br>Audio & Speech Research Group is a research group under the Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of Sciences. <br />The Principal Investigator of this research group is Prof. Feiran Yang (https://people.ucas.ac.cn/~feiran). Our research interests include Blind Source Separation, Array Signal Processing, Voice Anti-Spoofing, Acoustic Echo Cancellation, Speech Enhancement. Contact us: feiran@mail.ioa.ac.cn</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ioa-audio" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ioa-audio" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:feiran@mail.ioa.ac.cn" title="E-Mail → mailto:feiran@mail.ioa.ac.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">IOA-Audio</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
